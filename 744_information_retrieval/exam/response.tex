\documentclass[11pt]{article}

\usepackage{custom}
\setlength{\abovedisplayskip}{-15pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}m{#1}}

\title{605.744: Information Retrieval \\ Final Exam}
\author{Sabbir Ahmed}
\date{\today}

\begin{document}

\maketitle
\begin{enumerate}

  \item \begin{enumerate}
          \item Suppose the document collection is very large, and that the index will not fit in the available RAM. Describe an indexing algorithm that works when memory is small compared to the size of the index.

                \textbf{Answer:} One of the methods that were used in my implementation of the document ranking assignment to reduce memory overhead was to split up the pipeline into chunks so that the RAM would not get exhausted. The words needed to be extracted and sorted lexicographically, which can easily explode in memory requirements. One method employed was to gather as much of the term frequency of each of the terms in the documents and then write them out to disk. Once all of the term-docID-term frequency records were extracted across several files, they can be merged in a separate execution when the RAM usage is low. This final sorted file of the term-docID-term frequency records can later be used to traverse through and build the index file.

          \item Once an inverted file has been created, it is possible to calculate document vector lengths for a TF/IDF cosine model. This pre-calculation makes query-time performance much more efficient. Explain how right after creating the inverted file document vector lengths can be efficiency computed for all docids in parallel using one traversal (i.e., one single pass) over the inverted file.

                \textbf{Answer:} Pre-calculating the document vector lengths was also utilized in my implementation of the document ranking assignment. Computing the cosine similarity of documents against a query requires computing dot products with sums of squares of the TF-IDF values of each terms in a dictionary, which can be very computationally expensive. However, instead of computing the sums of squares of all the terms every time a query is inputted, they can be pre-calculated independently. Once an inverted file is created, the system can loop through each of the words in the dictionary and retrieve its information from the index file, which contains the corresponding term frequencies required to compute the square of the TF-IDF values. A separate hash table that maps the docIDs to their partial length is created. The squared TF-IDF values of each of the terms in a document get added to the corresponding docID in the hash table until the end of the dictionary is reached. This hash table can finally be written to disk and later retrieved to compute the dot products against the queries.

        \end{enumerate}


        % \item For this problem consider the following collection of 8 documents. No other documents are present in the
        %       collection besides these eight. When analyzing these documents and the query below you are to ignore all
        %       punctuation and any word with four or fewer letters. All short words with four or fewer letters are considered
        %       stopwords that are completely ignored for this problem. Use base 2 logs if any logarithm is required.
        %       (a) Compute cosine values and rank documents D1 and D2 using query Q using the vector cosine model with
        %       TF/IDF term weighting. The query Q consists of the words: "france aids ukraine". Show your work. Report
        %       scores to three decimal places (e.g., 0.123)
        %       (b) Now rank the same documents (D1 and D2) but this time by probability of relevance to the query using a
        %       unigram statistical language model. For smoothing purposes you should use a mixture model with a parameter
        %       l = 0.2. You should assume that the prior probability of relevance is the same for each document. Report
        %       scores using three digits of precision (e.g., 1.23 x 10^-4)
        %       D1: france sends help to ukraine
        %       D2: ukraine economy at risk
        %       D3: france and germany meet on ukraine war
        %       D4: economy tanks on inflation fear
        %       D5: france to talk on economy
        %       D6: france raises cheese prices
        %       D7: economy: inflation makes home prices soar
        %       D8: germany sends tanks to ukraine
        %       Query Q for both parts (a) and (b): "france aids ukraine".

        \begin{simptable}
          {Cardinalities of set-difference sets with various n-grams and normalization parameters}
          {scores}
          {| c | c | c | c |}
          \textbf{N-Gram} & \textbf{Normalized} & \textbf{$|G-O|$} & \textbf{$|O-G|$}
          \\ \hline
          \textbf{6} & True & 10 & 12
          \\ \hline
          \textbf{6} & False & 10 & 12
          \\ \hline
          \textbf{5} & True & 3 & 6
          \\ \hline
          \textbf{5} & False & 4 & 9
          \\ \hline
          \textbf{4} & True & 3 & 6
          \\ \hline
          \textbf{4} & False & 3 & 6
          \\ \hline
          \textbf{3} & True & 2 & 4
          \\ \hline
          \textbf{3} & False & 2 & 4
          \\ \hline
          \textbf{2} & True & 1 & 2
          \\ \hline
          \textbf{2} & False & 3 & 2
          \\ \hline
          \textbf{1} & True & 2 & 1
          \\ \hline
          \textbf{1} & False & 2 & 2
          \\ \hline
        \end{simptable}

        % numbner 5
  \item The three major problems in text retrieval are: (a) polysemy; (b) synonymy; and, (c) morphology. Briefly explain each issue and how it can lower performance. Give an example of each phenomena.

        \textbf{Answer:} \begin{enumerate}
          \item Polysemy refers to words that can have multiple meanings depending on the context. For example, \textit{space} can refer to its noun version of unoccupied area. The unoccupied area can be physical or abstract, i.e. "the space between planets" or "a teenager needing their own personal space". The word can also be used as a verb to refer to a person physically or emotionally distancing themselves from a situation, i.e. "spacing out during lectures".
                Polysemy introduces ambiguity to a retrieval if a query is not given enough context and the system retrieves the undesired version of the word.

          \item Synonymy refers to different words addressing the same meaning. For example, \textit{colossal}, \textit{giant} and \textit{huge} all describe the size of an object to be very big. Numerous other words also act as synonyms for \textit{big}.
                Synonymy can lower performance of a retrieval system if it is not aware of the numerous synonyms a query word may have. If a user queries for "big company" but the system only contains documents with the numerous synonyms of \textit{big}, the ranked documents may not be what the user implied.

          \item Morphology refers to the various conjugations of a word. In English, adding suffixes such as "s" or "es" transforms a noun into its plural form. Adding suffixes such as "d", "ed", and "ing" transforms a present tense verb to a different tense. Morphology is not only limited to suffixes or prefixes, since there are special cases of words needing a replacement in a character, i.e. \textit{sang} is the past tense form of \textit{swim}, while \textit{sung} is its past participle form.
                Morphology can introduce issues in a retrieval system if the different variations of the words in a query are not accounted for. These systems often employ some levels of stemming in their dictionary and the query processing to normalize the words to their base forms. However, stemming can introduce additional ambiguity when different words get stemmed to a common word. i.e. "transparent" and "transparency" get stemmed to "transpar" using a Porter stemmer.
        \end{enumerate}

        % number 6
  \item Short answers about text classification.
        \begin{enumerate}
          \item What is negative evidence in Binomial (also called Bernoulli) Naïve Bayes text classification?

                \textbf{Answer:}

          \item For a class that attains precision of 0.5 and recall of 0.6, what is the corresponding F1 score?

                \textbf{Answer:} The F1 score is computed as $2 \times \frac{P\times R}{P+R}$. Therefore,
                \begin{align*}
                  F1 & = 2 \times \frac{P\times R}{P+R}         \\
                     & = 2 \times \frac{0.5\times 0.6}{0.5+0.6} \\
                     & = 0.54
                \end{align*}

          \item For the three classes below (business, local, and sports) with the indicated system predictions, calculate precision for the 12 news articles in two ways: using micro averaging and macro averaging.

                \textbf{Answer:} Precision is the ratio of the true positives over the total number of classes labeled positive (both true and false positive classes). In the table, there are a total of 8 true positives, with 2, 3, and 3 true positives and 2, 0, and 2 false positives in the 3 respective classes.

                The micro average, $P_{\mu}$, can be computed as:
                \begin{align*}
                  P_{\mu} & = \frac{\sum_{i=0}^{n}(TP_i)}{\sum_{i=0}^{n}(TP_i+FP_i)} \\
                          & = \frac{2+3+3}{(2+2)+(3+0)+(3+2)}                        \\
                          & = \frac{8}{12}                                           \\
                          & = 0.67
                \end{align*}

                The macro average, $P_{M}$, is computed by taking the expected value of the individual precision scores of the 3 classes.
                \begin{align*}
                  P_{b} & = \frac{TP_i}{TP_i+FP_i}      \\
                        & = \frac{2}{4}                 \\
                        & = 0.5                         \\
                  P_{l} & = \frac{TP_i}{TP_i+FP_i}      \\
                        & = \frac{3}{3}                 \\
                        & = 1                           \\
                  P_{s} & = \frac{TP_i}{TP_i+FP_i}      \\
                        & = \frac{3}{5}                 \\
                        & = 0.6                         \\
                  P_{M} & = \frac{P_{b}+P_{l}+P_{s}}{3} \\
                        & = \frac{0.5+1+0.6}{3}         \\
                        & = 0.70
                \end{align*}

        \end{enumerate}

        % number 7
  \item Give brief responses to the following:
        \begin{enumerate}
          \item Give one example when converting all upper-case letters to lower-case could cause a retrieval error.

                \textbf{Answer:} Acronyms are almost always consisting of all capital letters. They are used as proper nouns referring to organizations or businesses. If

          \item True or False: stemming can improve recall at rank 100 in a TF/IDF vector cosine system?

                \textbf{Answer:}

          \item What is a permuterm index useful for?

                \textbf{Answer:} https://nlp.stanford.edu/IR-book/html/htmledition/permuterm-indexes-1.html

          \item What modification to an inverted file data structure makes it possible to intersect two postings lists for a Boolean AND query in less than linear time in the sum of the lengths of the two postings lists?

                \textbf{Answer:}

          \item What is front-coding and what is it used for in information retrieval?

                \textbf{Answer:} Front-coding is a method of text compression that utilizes prefixes from the previous word in the dictionary to avoid duplicating portions of the words. For example, if a dictionary contains the words \textit{trova}, \textit{trovano} and \textit{trovare}, then the compression string would use \textit{trova} as the base prefix, and append the remnants of the individual strings to represent them, i.e. 5trova*no2$\Diamond$re instead of trova/trovano/trovare. This method works best if the dictionary is lexicographically sorted.

          \item In a large collection of English documents, which word would you expect to have a lower inverse document frequency (IDF), apple or volcanic? Why?

                \textbf{Answer:} The more frequent a word appears across documents, the lower its IDF value tends to be. I would expect \textit{apple} to appear a lot more than \textit{volcanic} in documents in a general corpus. However, if the documents are in a corpus related to topics on geology or seismic activities, I'd expect \textit{volcanic} to appear more frequently across the documents and thus having a lower IDF value.

          \item What are the two main principles behind cover density ranking?

                \textbf{Answer:}

          \item Explain Broder's taxonomy of information needs.

                \textbf{Answer:}

          \item How are estimates of p(word|class) calculated in Multinomial Naïve Bayes text classification?

                \textbf{Answer:}

          \item What is the kernel trick?

                \textbf{Answer:}

        \end{enumerate}

\end{enumerate}

\end{document}
