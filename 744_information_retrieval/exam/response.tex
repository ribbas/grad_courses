\documentclass[11pt]{article}

\usepackage{custom}
\usepackage{enumitem}
\setlength{\abovedisplayskip}{-15pt}
\setlength{\belowdisplayskip}{0pt}
\setlength{\abovedisplayshortskip}{0pt}
\setlength{\belowdisplayshortskip}{0pt}
\newcolumntype{R}[1]{>{\raggedleft\arraybackslash}m{#1}}

\title{605.744: Information Retrieval \\ Final Exam}
\author{Sabbir Ahmed}
\date{\today}

\begin{document}

\maketitle
\begin{enumerate}

  \item \begin{enumerate}
          \item Suppose the document collection is very large, and that the index will not fit in the available RAM. Describe an indexing algorithm that works when memory is small compared to the size of the index.

                \textbf{Answer:} One of the methods that were used in my implementation of the document ranking assignment to reduce memory overhead was to split up the pipeline into chunks so that the RAM would not get exhausted. The words needed to be extracted and sorted lexicographically, which can easily explode in memory requirements. One method employed was to gather as much of the term frequency of each of the terms in the documents and then write them out to disk. Once all of the term-docID-term frequency records were extracted across several files, they can be merged in a separate execution when the RAM usage is low. This final sorted file of the term-docID-term frequency records can later be used to traverse through and build the index file.

          \item Once an inverted file has been created, it is possible to calculate document vector lengths for a TF/IDF cosine model. This pre-calculation makes query-time performance much more efficient. Explain how right after creating the inverted file document vector lengths can be efficiency computed for all docids in parallel using one traversal (i.e., one single pass) over the inverted file.

                \textbf{Answer:} Pre-calculating the document vector lengths was also utilized in my implementation of the document ranking assignment. Computing the cosine similarity of documents against a query requires computing dot products with sums of squares of the TF-IDF values of each terms in a dictionary, which can be very computationally expensive. However, instead of computing the sums of squares of all the terms every time a query is inputted, they can be pre-calculated independently. Once an inverted file is created, the system can loop through each of the words in the dictionary and retrieve its information from the index file, which contains the corresponding term frequencies required to compute the square of the TF-IDF values. A separate hash table that maps the docIDs to their partial length is created. The squared TF-IDF values of each of the terms in a document get added to the corresponding docID in the hash table until the end of the dictionary is reached. This hash table can finally be written to disk and later retrieved to compute the dot products against the queries.

        \end{enumerate}


  \item For this problem consider the following collection of 8 documents. No other documents are present in the collection besides these eight. When analyzing these documents and the query below you are to ignore all punctuation and any word with four or fewer letters. All short words with four or fewer letters are considered stopwords that are completely ignored for this problem. Use base 2 logs if any logarithm is required.

        \textbf{Answer:}

        The documents were normalized to the following:

        \begin{enumerate}[label=D\arabic*:]
          \item france sends ukraine
          \item ukraine economy
          \item france germany ukraine
          \item economy tanks inflation
          \item france economy
          \item france raises cheese prices
          \item economy inflation makes prices
          \item germany sends tanks ukraine
        \end{enumerate}

        Query Q for both parts (a) and (b): "france ukraine".

        \begin{enumerate}
          \item Compute cosine values and rank documents D1 and D2 using query Q using the vector cosine model with TF/IDF term weighting. The query Q consists of the words: "france aids ukraine". Show your work. Report scores to three decimal places (e.g., $0.123$)

                \textbf{Answer:}
                Compiling the term frequencies of all the relevant vocabulary:
                \begin{simptable}
                  {Term frequencies of \textit{france sends ukraine economy}}
                  {scores}
                  {| c | c | c | c |}
                  \textbf{Word} & \textbf{D1} & \textbf{D2} & Q \\
                  \hline
                  france  & 1 & 0 & 1 \\
                  \hline
                  sends   & 1 & 0 & 0 \\
                  \hline
                  ukraine & 1 & 1 & 1 \\
                  \hline
                  economy & 0 & 1 & 0 \\
                  \hline
                \end{simptable}
                The TF and IDF of the vocabulary are computed using the following values:
                \begin{align*}
                  TF(t)  & = \text{term frequency in the corpus} \\
                  IDF(t) & = \text{inverse document frequency}   \\
                         & = log_2\left(\frac{N}{df(t)}\right),  \\
                         & \text{where:}                         \\
                         & N=\text{length of corpus}=8,          \\
                         & df(t)=\text{document frequency}
                \end{align*}
                {\renewcommand{\arraystretch}{1.5}
                \begin{simptable}
                  {TF-IDF of \textit{france sends ukraine economy}}
                  {s}
                  {| c | c | c |}
                  \textbf{term} & \textbf{DF} & \textbf{IDF}                      \\
                  \hline
                  france  & 4 & $log_2\left(\frac{8}{4}\right)=1$ \\
                  \hline
                  sends   & 2 & $log_2\left(\frac{8}{2}\right)=2$ \\
                  \hline
                  ukraine & 4 & $log_2\left(\frac{8}{4}\right)=1$ \\
                  \hline
                  economy & 4 & $log_2\left(\frac{8}{4}\right)=1$ \\
                  \hline
                \end{simptable}}
                \begin{simptable}
                  {Squares of TF-IDF}
                  {scores}
                  {| c | c | c | c |}
                  \textbf{Word} & \textbf{D1} & \textbf{D2} & Q \\
                  \hline
                  france  & 1 & 0 & 1 \\
                  \hline
                  sends   & 4 & 0 & 0 \\
                  \hline
                  ukraine & 1 & 1 & 1 \\
                  \hline
                  economy & 0 & 1 & 0 \\
                  \hline
                \end{simptable}
                Finally, computing the cosine similarities.
                  {\renewcommand{\arraystretch}{1.5}
                    \begin{simptable}
                      {Similarities}
                      {s}
                      {| c | c | c | c |}
                      \textbf{value}  & \textbf{D1} & \textbf{D2} & \textbf{Q}                                              \\
                      \hline
                      sum of squares  & 6 & 2 & 2 \\
                      \hline
                      length, $||D|| = \sqrt{\sum_{i=1}^{t}{w_i^2}}$ & 2.4494 & 1.4142 & 1.4142 \\
                      \hline
                      dot product     & 2 & 1 & 2  \\
                      \hline
                      cosine similarity, $\frac{d \cdot q}{||d||*||q||}$ & $\frac{2}{2.4494*1.4142}=0.577$ & $\frac{1}{1.4142*1.4142}=0.500$ & 1 \\
                      \hline
                    \end{simptable}}
                \clearpage
                \newpage

          \item Now rank the same documents (D1 and D2) but this time by probability of relevance to the query using a unigram statistical language model. For smoothing purposes you should use a mixture model with a parameter $\lambda = 0.2$. You should assume that the prior probability of relevance is the same for each document. Report scores using three digits of precision (e.g., $1.23 \times 10^{-4}$)

                \textbf{Answer:} Computing the $tf_{t,d}$ (the (raw) term frequency of term $t$ in document $d$) and $L_d$ (the number of tokens in document $d$):

                \begin{simptable}
                  {}
                  {scores}
                  {|c|c|c|c|c|c|c|}
                  & \textbf{$tf_{D1}$} & \textbf{$L_{D1}$} & \textbf{$tf_{D2}$} & \textbf{$L_{D2}$} & \textbf{$cf_{t}$} & \textbf{$cs$}
                  \\ \hline
                  france  & 1 & 3 & 0 & 2 & 4 & 25
                  \\ \hline
                  ukraine & 1 & 3 & 1 & 2 & 4 & 25
                  \\ \hline
                \end{simptable}
                Using $\lambda = 0.2$ and Q=\textit{france ukraine}, and expanding the probability:
                \begin{align*}
                  P(d|q) & \propto P(d) \prod_{t \in q} ((1 - \lambda) P(t|M_c) + \lambda P(t|M_d))                            \\
                         & \propto \prod_{t \in q} \left((1 - \lambda) \frac{cf_{t}}{cs} + \lambda \frac{tf_{t,d}}{L_d}\right)
                \end{align*}
                Computing the probability for each documents:
                \begin{align*}
                  P(Q|D1) & = \prod \left((1 - \lambda) \frac{cf_{t}}{cs} + \lambda \frac{tf_{t,D1}}{L_{D1}}\right)                                                                                          \\
                          & = \prod \left(0.8\times \frac{cf_{t}}{25} + 0.2\times \frac{tf_{t,D1}}{3}\right)                                                                                                 \\
                          & = \left(0.8\times \frac{cf_{france}}{25} + 0.2\times \frac{tf_{france,D1}}{3}\right) \times \left(0.8\times \frac{cf_{ukraine}}{25} + 0.2\times \frac{tf_{ukraine,D1}}{3}\right) \\
                          & = \left(0.8\times \frac{4}{25} + 0.2\times \frac{1}{3}\right) \times \left(0.8\times \frac{4}{25} + 0.2\times \frac{1}{3}\right)                                                 \\
                          & = (0.194)(0.194)                                                                                                                                                                 \\
                          & = 3.79 \times 10^{-2}
                \end{align*}
                \begin{align*}
                  P(Q|D2) & = \prod \left((1 - \lambda) \frac{cf_{t}}{cs} + \lambda \frac{tf_{t,D2}}{L_{D2}}\right)                                                                                          \\
                          & = \prod \left(0.8\times \frac{cf_{t}}{25} + 0.2\times \frac{tf_{t,D2}}{3}\right)                                                                                                 \\
                          & = \left(0.8\times \frac{cf_{france}}{25} + 0.2\times \frac{tf_{france,D2}}{3}\right) \times \left(0.8\times \frac{cf_{ukraine}}{25} + 0.2\times \frac{tf_{ukraine,D2}}{3}\right) \\
                          & = \left(0.8\times \frac{4}{25} + 0.2\times \frac{0}{2}\right) \times \left(0.8\times \frac{4}{25} + 0.2\times \frac{1}{2}\right)                                                 \\
                          & = (0.128)(0.228)                                                                                                                                                                 \\
                          & = 2.92 \times 10^{-2}
                \end{align*}

        \end{enumerate}

        % 3
  \item Two IR systems (called A and B below) return top ranked lists of 20 documents for a query in an IR test collection. The query is known to have 10 relevant documents in the test collection. The test collection contains exactly 1,000 documents.
        System A finds relevant docs at ranks: 2, 4, 5, 12, 15, 20. The other top 20 documents are not relevant.
        System B finds relevant docs at ranks: 1, 2, 8, 10, 15, 18, 20. The other top 20 documents are not relevant.

        \begin{enumerate}
          \item Which system has higher Recall at rank 20?

                \textbf{Answer:} System B has the higher recall at rank 20 with 7 retrieved relevant documents.

          \item What is Precision at 10 documents (P@10) for both systems?

                \textbf{Answer:} For System A, the relevant documents at rank 10 retrieved are \{2,3,5\}. Therefore, the P@10 is 3/10=0.3.

                For System B, the relevant documents at rank 10 retrieved are \{1,2,8,10\}. Therefore, the P@10 is 4/10=0.4.

          \item What is the average precision (AP) of System A?

                \textbf{Answer:} (1/2 + 2/4 + 3/5 + 4/12 + 5/15 + 6/20)/10 = 0.257

          \item In general, what is the difference between mean average precision and average precision?

                \textbf{Answer:} The average precision computes the averages of the precision of a retrieval system over a range of recall values. The mean average precision refers to averaging the average precision values over multiple queries.

          \item Suppose System A is used to create a ranking of all 1,000 documents and the first 20 documents retrieved as the same as those listed above. What is the highest possible average precision score for System A computed over the entire ranking?

                \textbf{Answer:} The precision may increase after the first 20 documents with every one of its later retrievals being relevant.

        \end{enumerate}

  \item Short answer questions / calculations about index compression. Show your work and justify any responses.
        Note in working on this problem you should use the method for calculating gamma/delta codes and variable
        byte coding presented in the lecture (or textbook) and not any other variation.
        \begin{enumerate}
          \item Represent the integer 79 using Gamma coding.

                \textbf{Answer:} Converting to gamma coding:
                \begin{align*}
                  unary(floor(log_2(79)))         & = unary(6)               \\
                                                  & = 1111110                \\
                  binary(x-2^{(floor(log_2(x)))}) & = binary(79 - 2^6)       \\
                                                  & = binary(79 - 64)        \\
                                                  & = 001111                 \\
                  gamma(79)                       & = 1 \ 1111 \ 1000 \ 1111
                \end{align*}


          \item Why do we encode docid gaps instead of raw docids in postings lists?

                \textbf{Answer:} DocID gaps take up a significantly less storage than entire DocIDs. For example, encoding the DocIDs \{1,7,1000,1001\} can be compressed with their DocID gaps as \{1,6,994,1\}.

          \item The following Gamma-encoded bit string represents a gap list of 4 docids. What are the docids?

                11111010101110001001110010

                \textbf{Answer:} Regrouping the bit sequence using the following algorithm:
                \begin{itemize}
                  \item Read the 1 bits until the first zero and store it as $magnitude$ of the unary code
                  \item Read the $magnitude$ number of bits as $b=\text{binary code}$
                  \item Compute $u=2^{magnitude}$
                  \item Add $u$ to the decimal value of the binary code, $u + dec(b)$
                \end{itemize}
                Therefore,
                \begin{align*}
                   & \{111110 10101 \ 110 00 \ 10 0 \ 1110 010 \}                               \\
                   & = \{2^5 + dec(10101), \ 2^2 +  dec(00), \ 2^1 + dec(0), \ 2^3 + dec(010)\} \\
                   & = \{2^5 + 21, \ 2^2 +  0, \ 2^1 + 0, \ 2^3 + 2\}                           \\
                   & = \{53, 4, 2, 10\}
                \end{align*}
                The docIDs are:
                \begin{align*}
                   & = \{53, 53+4, 53+4+2, 53+4+2+10\} \\
                   & = \{53, 57, 59, 69\}
                \end{align*}

          \item Term "JHU" occurs in four documents with docids: 64, 100, 2000, and 2032. Calculate a compressed representation of this four docid posting list using variable byte coding. Hint: you should encode gaps, not raw docids.

                \textbf{Answer:} The gap list is \{64,36,1900,32\}.

                For (e-g) indicate True or False -- and Justify Your Response.
          \item Claim: Gamma codes are always an odd number of bits in length.

                \textbf{Answer:} True, since gamma codes are concatenations of the unary and binary representations of a number, the binary code will be even if the unary code is odd and vice versa.

          \item Claim: A Gamma code always starts with a one bit

                \textbf{Answer:} False, since the unary code of 1 would be 0.

          \item Claim: The Gamma code for the integer $2^{51} - 1000$ (read: two raised to the fifty-first power minus one
                thousand) is shorter than 85 bits.

                \textbf{Answer:} False, since the unary code of the integer would be 51 bits in length, the binary portion will be close to that length and add up to more than 85 bits.

        \end{enumerate}

        % 5
  \item The three major problems in text retrieval are: (a) polysemy; (b) synonymy; and, (c) morphology. Briefly explain each issue and how it can lower performance. Give an example of each phenomena.

        \textbf{Answer:} \begin{enumerate}
          \item Polysemy refers to words that can have multiple meanings depending on the context. For example, \textit{space} can refer to its noun version of unoccupied area. The unoccupied area can be physical or abstract, i.e. "the space between planets" or "a teenager needing their own personal space". The word can also be used as a verb to refer to a person physically or emotionally distancing themselves from a situation, i.e. "spacing out during lectures".
                Polysemy introduces ambiguity to a retrieval if a query is not given enough context and the system retrieves the undesired version of the word.

          \item Synonymy refers to different words addressing the same meaning. For example, \textit{colossal}, \textit{giant} and \textit{huge} all describe the size of an object to be very big. Numerous other words also act as synonyms for \textit{big}.
                Synonymy can lower performance of a retrieval system if it is not aware of the numerous synonyms a query word may have. If a user queries for "big company" but the system only contains documents with the numerous synonyms of \textit{big}, the ranked documents may not be what the user implied.

          \item Morphology refers to the various conjugations of a word. In English, adding suffixes such as "s" or "es" transforms a noun into its plural form. Adding suffixes such as "d", "ed", and "ing" transforms a present tense verb to a different tense. Morphology is not only limited to suffixes or prefixes, since there are special cases of words needing a replacement in a character, i.e. \textit{sang} is the past tense form of \textit{swim}, while \textit{sung} is its past participle form.
                Morphology can introduce issues in a retrieval system if the different variations of the words in a query are not accounted for. These systems often employ some levels of stemming in their dictionary and the query processing to normalize the words to their base forms. However, stemming can introduce additional ambiguity when different words get stemmed to a common word. i.e. "transparent" and "transparency" get stemmed to "transpar" using a Porter stemmer.
        \end{enumerate}

        % number 6
  \item Short answers about text classification.
        \begin{enumerate}
          \item What is negative evidence in Binomial (also called Bernoulli) Naïve Bayes text classification?

                \textbf{Answer:} That the word does not occur in the classes.

          \item For a class that attains precision of 0.5 and recall of 0.6, what is the corresponding F1 score?

                \textbf{Answer:} The F1 score is computed as $2 \times \frac{P\times R}{P+R}$. Therefore,
                \begin{align*}
                  F1 & = 2 \times \frac{P\times R}{P+R}         \\
                     & = 2 \times \frac{0.5\times 0.6}{0.5+0.6} \\
                     & = 0.54
                \end{align*}

          \item For the three classes below (business, local, and sports) with the indicated system predictions, calculate precision for the 12 news articles in two ways: using micro averaging and macro averaging.

                \textbf{Answer:} Precision is the ratio of the true positives over the total number of classes labeled positive (both true and false positive classes). In the table, there are a total of 8 true positives, with 2, 3, and 3 true positives and 2, 0, and 2 false positives in the 3 respective classes.

                The micro average, $P_{\mu}$, can be computed as:
                \begin{align*}
                  P_{\mu} & = \frac{\sum_{i=0}^{n}(TP_i)}{\sum_{i=0}^{n}(TP_i+FP_i)} \\
                          & = \frac{2+3+3}{(2+2)+(3+0)+(3+2)}                        \\
                          & = \frac{8}{12}                                           \\
                          & = 0.67
                \end{align*}

                The macro average, $P_{M}$, is computed by taking the expected value of the individual precision scores of the 3 classes.
                \begin{align*}
                  P_{b} & = \frac{TP_i}{TP_i+FP_i}      \\
                        & = \frac{2}{4}                 \\
                        & = 0.5                         \\
                  P_{l} & = \frac{TP_i}{TP_i+FP_i}      \\
                        & = \frac{3}{3}                 \\
                        & = 1                           \\
                  P_{s} & = \frac{TP_i}{TP_i+FP_i}      \\
                        & = \frac{3}{5}                 \\
                        & = 0.6                         \\
                  P_{M} & = \frac{P_{b}+P_{l}+P_{s}}{3} \\
                        & = \frac{0.5+1+0.6}{3}         \\
                        & = 0.70
                \end{align*}

        \end{enumerate}

        % number 7
  \item Give brief responses to the following:
        \begin{enumerate}
          \item Give one example when converting all upper-case letters to lower-case could cause a retrieval error.

                \textbf{Answer:} Acronyms are almost always consisting of all capital letters. They are used as proper nouns referring to organizations or businesses. If

          \item True or False: stemming can improve recall at rank 100 in a TF/IDF vector cosine system?

                \textbf{Answer:} True, because the less frequent words would become stemmed to more common words.

          \item What is a permuterm index useful for?

                \textbf{Answer:} Permuterms are used in wildcard queries to indicate the end of a term. This is needed because when creating rotation permutations of the vocabulary, the original word may become lost. The permuterm index can be used to rotate the word back to its original form.

          \item What modification to an inverted file data structure makes it possible to intersect two postings lists for a Boolean AND query in less than linear time in the sum of the lengths of the two postings lists?

                \textbf{Answer:} By adding the file offsets to the index file.

          \item What is front-coding and what is it used for in information retrieval?

                \textbf{Answer:} Front-coding is a method of text compression that utilizes prefixes from the previous word in the dictionary to avoid duplicating portions of the words. For example, if a dictionary contains the words \textit{trova}, \textit{trovano} and \textit{trovare}, then the compression string would use \textit{trova} as the base prefix, and append the remnants of the individual strings to represent them, i.e. 5trova*no2$\Diamond$re instead of trova/trovano/trovare. This method works best if the dictionary is lexicographically sorted.

          \item In a large collection of English documents, which word would you expect to have a lower inverse document frequency (IDF), apple or volcanic? Why?

                \textbf{Answer:} The more frequent a word appears across documents, the lower its IDF value tends to be. I would expect \textit{apple} to appear a lot more than \textit{volcanic} in documents in a general corpus. However, if the documents are in a corpus related to topics on geology or seismic activities, I'd expect \textit{volcanic} to appear more frequently across the documents and thus having a lower IDF value.

          \item What are the two main principles behind cover density ranking?

                \textbf{Answer:} The covers for the query contain the shortest interval between the terms and the documents have to be concatenated together.

          \item Explain Broder's taxonomy of information needs.

                \textbf{Answer:} Broder's taxonomy refers to the classification of search queries by users into 3 categories: informational, navigational, and transactional. The system can provide higher quality documents if it can classify the users' query.

          \item How are estimates of p(word|class) calculated in Multinomial Naïve Bayes text classification?

                \textbf{Answer:} With the product of the number of occurrences of the word in each of the classes.

          \item What is the kernel trick?

                \textbf{Answer:} The kernel trick refers to converting data points into vectors.

        \end{enumerate}

\end{enumerate}

\end{document}
