\documentclass[11pt]{article}

\usepackage{appendix}
\usepackage{array}
\usepackage{caption}
\usepackage{parskip}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{listings}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\captionsetup[table]{skip=5pt}
\newcolumntype{L}[1]{>{\arraybackslash}m{#1}}

\usepackage{pythonhighlight}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{605.744: Information Retrieval \\ Programming Assignment \#4: Binary Text Classification}
\author{Sabbir Ahmed}
\date{\today}

\begin{document}
\maketitle	
\tableofcontents
\clearpage
\newpage

\section*{Introduction}
This paper describes the classification of the Systematic Review dataset through exploratory analysis, grid searching for optimal estimators and their hyper-parameters, and determining the best features.

\section*{Technical Background}
All of the source code is in Python 3.10. The program is split into several modules and follows an object oriented structure. The following is the directory structure of the source code:

% .
% ├── ir/
% │   ├── __init__.py
% │   ├── const.py
% │   ├── files.py
% │   ├── metrics.py
% │   ├── model.py
% │   └── normalize.py
% ├── models/
% ├── outputs/
% └── run.py

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.2]{statics/dirtree.png}
    \caption{Directory Hierarchy of Assignment 4}
\end{figure}

The source code for all of the files are attached in Appendix \ref{appendix:src}.

The total number of non-empty lines of code for the program totals to under 750.

\begin{figure}[!ht]
    \includegraphics[scale=0.35]{statics/uml.png}
    \centering
    \caption{UML of Information Retrieval}
\end{figure}

\subsection*{Classes}
\texttt{scikit-learn} was used to implement portions of this assignment.

Some classes from Assignment 3 were used in this project:
\begin{itemize}
    \item the driver script \texttt{run.py} was modified with the relevant flags
    \item the methdods in \texttt{files.IO} were simplified to handle only plain text and joblib binaries
    \item the \texttt{files.CorpusFile} class was modified to process TSV files and transform the content into a list of mapped values
    \item the \texttt{normalize.Normalizer} class was used to compare text tokenization methods
    \item the \texttt{lexer} classes were replaced by the \texttt{CountVectorizer} class provided by \texttt{scikit-learn}
\end{itemize}

\section*{Exploratory Analysis}
The dataset is a collection of tab separated value (TSV) files with 10 features.

\begin{table}[!ht]
    \caption{Description of the features of the Systematic Review dataset}
    \begin{center}

        \begin{tabular}{| l | l |}
        \hline
        \textbf{Feature} & \textbf{Description}
        \\ \hline
        Assessment & -1, 0, or 1; zero indicates unknown; 1=accept, -1=reject
        \\ \hline
        DocID & Unique ID. Usually PubMed ID, or hashed ID
        \\ \hline
        Title & Article title
        \\ \hline
        Authors & List of authors (poss. separated with ";"). Unnormalized.
        \\ \hline
        Journal & Journal title
        \\ \hline
        ISSN & Numeric code for journal (possibly a list)
        \\ \hline
        Year & Publication year
        \\ \hline
        language & Trigram for language code (e.g., "eng")
        \\ \hline
        Abstract & Several sentence abstract from article
        \\ \hline
        Keywords & List of keywords (formatting is highly variable) - should be ";" separated.
        \\ \hline
        \end{tabular}

    \end{center}

\end{table}

The \textit{Assessment} feature in the training and development datasets is the target binary value, and will be predicted for the testing dataset. The feature is heavily imbalanced, with Table \ref{table:dist_train} showing the distribution in the training dataset:

\begin{table}[!ht]
    \caption{Distribution of \textit{Assessment} in the training dataset}
    \label{table:dist_train}
    \begin{center}

        \begin{tabular}{| c | c |}
        \hline
        \textbf{Value} & \textbf{Count}
        \\ \hline
        -1 & 21,000
        \\ \hline
        1 & 700
        \\ \hline
        \end{tabular}

    \end{center}

\end{table}

The following features are considered text features, where they are represented as single strings:
\begin{itemize}
    \item DocID
    \item Title
    \item Language
    \item Abstract
\end{itemize}

The following features are considered list of text features, where they are represented as string lists:
\begin{itemize}
    \item Authors
    \item Journal
    \item ISSN
    \item Keywords
\end{itemize}

The remaining (optional) feature, \textit{Year}, is represented as an integer.

\section*{Classification Algorithms}
For classification, machine learning algorithms implemented by \texttt{scikit-learn} were used.

For text tokenizing and normalizing, the \texttt{CountVectorizer} and \texttt{TfidfTransformer} classes by \texttt{scikit-learn} were used. \texttt{CountVectorizer} ingested text values and created bags-of-words. This data structure was further transformed using \texttt{TfidfTransformer} to assign TF-IDF values to the terms in the vocabulary.

\subsection*{Scoring}
To compare performances of the models, the following metrics were emphasize:
\begin{itemize}
\setlength\itemsep{1em}
    \item Precision (P): $ \dfrac{TP}{TP + FP} $
    \item Recall (R): $ \dfrac{TP}{TP + FN} $
    \item F1-score: $ 2 \cdot \dfrac{P \cdot R}{P+R} $
\end{itemize}

These scores are computed in \texttt{metrics.Metrics}.

\section*{Experiments}

\subsection*{Baseline}
For the initial phase, only the \textit{Title} feature was used as the feature. As a text feature, each of the values were tokenized, vectorized, and their TF-IDF values were used to predict the corresponding target value. The previously implemented \texttt{normalize.Normalizer} class was used as the tokenizer to \texttt{CountVectorizer}.

The text vectorizers from \texttt{scikit-learn} were used with their default parameters. For classification, the linear support vector machine (SVM) with stochastic gradient descent (SGD), \texttt{SGDClassifier(loss="hinge")} was used.

To account for the skewed data, an additional parameter $class\_weight=\{1:30\}$ was used.

Table \ref{table:base_score} lists the model's scores:
\begin{table}[!ht]
    \caption{Scores using only \textit{Title} as the feature}
    \label{table:base_score}
    \begin{center}

        \begin{tabular}{| c | c |}
        \hline
        \textbf{Metric} & \textbf{Score}
        \\ \hline
        Precision & 0.195 
        \\ \hline
        Recall & 0.640
        \\ \hline
        F1-Score & 0.299
        \\ \hline
        \end{tabular}

    \end{center}

\end{table}

\subsection*{Experiment \#1}
To improve the scores of the model, the features \textit{Abstract} and \textit{Keywords} were added. The vocabulary from the 3 features were merged and tokenized through the vectorizers. 

Table \ref{table:gs0_score} lists the model's scores:
\begin{table}[!ht]
    \caption{Scores using \textit{Title}, \textit{Abstract} and \textit{Keywords} as the features}
    \label{table:gs0_score}
    \begin{center}

        \begin{tabular}{| c | c |}
        \hline
        \textbf{Metric} & \textbf{Score}
        \\ \hline
        Precision & 0.330 
        \\ \hline
        Recall & 0.747
        \\ \hline
        F1-Score & 0.458
        \\ \hline
        \end{tabular}

    \end{center}

\end{table}

\subsection*{Experiment \#2}
In addition to the features, the grid searching algorithm by scikit-learn, \texttt{GridSearchCV}, was used to find the best-performing hyper-parameters. The following parameters were used:
\begin{enumerate}
    \item Tokenizer for \texttt{CountVectorizer}:
    \begin{enumerate}
            \item None
            \item \texttt{Normalizer()}
            \begin{enumerate}
                \item With one of the following stemmers:
                \begin{enumerate}
                    \item Snowball stemmer
                    \item Porter stemmer
                \end{enumerate}
                \item Combined with the following stop words list options:
                \begin{enumerate}
                    \item No stopwords removed
                    \item Custom stop words list generated from previous assignments
                    \item English stop words provided by scikit-learn
                \end{enumerate}
            \end{enumerate}
        \end{enumerate}
    \item Class weight for \texttt{SGDClassifier}: $\{\{1: i\}, 3 \le i \le 30\}$
\end{enumerate}
In total, 196 combinations were exhaustively searched to find the optimal scores.

Table \ref{table:gs1_param} lists the parameters with the best scores.
\begin{table}[!ht]
    \caption{Optimal parameters determined via grid search}
    \label{table:gs1_param}
    \begin{center}

        \begin{tabular}{| l | c |}
        \hline
        \textbf{Parameter} & \textbf{Value}
        \\ \hline
        Tokenizer for \texttt{CountVectorizer} & None
        \\ \hline
        Stop words list & None
        \\ \hline
        Class weight for \texttt{SGDClassifier} & \{1: 3\}
        \\ \hline
        \end{tabular}

    \end{center}

\end{table}

Along with the hyper-parameter search, various combinations of features were tested as well. Features such as \textit{Authors} and \textit{Year} do not appear to contribute to the model's performance, and \textit{Journal} appears to degrade the performance. The optimal features were determined to be: \{\textit{Title}, \textit{Abstract}, \textit{Keywords}, \textit{Language}\}.

Table \ref{table:gs1_score} shows the model's scores:
\begin{table}[!ht]
    \caption{Scores using \textit{Title}, \textit{Abstract}, \textit{Keywords} and \textit{Language} as the features}
    \label{table:gs1_score}
    \begin{center}

        % cv__tokenizer: None
        % clf__class_weight: {1: 12}
        % Dumped model to 'models/model-2.joblib'
        % test targets: {-1: 4700, 1: 150}
        % predicted targets: {-1: 4635, 1: 215}
        % {'precision': 0.47, 'recall': 0.673, 'f1': 0.553}
        
        % cv__tokenizer: Normalizer/snowball/None
        % clf__class_weight: {1: 4}
        % Dumped model to 'models/model-2.joblib'
        % test targets: {-1: 4700, 1: 150}
        % predicted targets: {-1: 4675, 1: 175}
        % {'precision': 0.503, 'recall': 0.587, 'f1': 0.542}
        
        \begin{tabular}{| c | c |}
        \hline
        \textbf{Metric} & \textbf{Score}
        \\ \hline
        Precision & 0.633 
        \\ \hline
        Recall & 0.460
        \\ \hline
        F1-Score & 0.533
        \\ \hline
        \end{tabular}

    \end{center}

\end{table}

\subsubsection*{Analysis}
The exhaustive grid search still yielded a model with poor scores. The 

\subsection*{Experiment \#3}

% scale_pos_weight": [3]
% "learning_rate": 0.2,
% "n_estimators": 100,
% "objective": "binary:logistic",
% "max_depth": 3,
% "min_child_weight": 10,
% "max_delta_step": 3,
% "subsample": 0.8,

% {'precision': 0.631, 'recall': 0.433, 'f1': 0.514}

\subsubsection*{Analysis}
The final scores of the 

\appendix
\addcontentsline{toc}{part}{Appendix}%

\section{Source Code} \label{appendix:src}

\inputpython{../ir/\_\_init\_\_.py}{ir/\_\_init\_\_.py}
\inputpython{../ir/files.py}{ir/files.py}
\inputpython{../ir/metrics.py}{ir/metrics.py}
\inputpython{../ir/normalize.py}{ir/normalize.py}
\inputpython{../ir/model.py}{ir/model.py}

\inputpython{../run.py}{run.py}

\end{document}
