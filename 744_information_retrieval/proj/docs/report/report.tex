\documentclass[11pt]{article}

\usepackage{custom}
\newcolumntype{R}[1]{>{\centering\arraybackslash}m{#1}}

\title{605.744: Information Retrieval \\ Emotion Extraction From Lyrics}
\author{Sabbir Ahmed}
\date{\today}

\begin{document}
\maketitle
\tableofcontents
\clearpage
\newpage

\begin{abstract}
  \noindent Mood classification in music has become more prevalent with the growing streaming industry. Categorizing songs based on the perceived emotions allow music streaming services to improve their recommendation systems and automatic playlist generation. Services such as Spotify use audio features such as duration, energy, tempo, etc. and other combinations of audio features such as danceability and instrumentalness to group similar tracks. These classifications focus solely on the audio production of the songs while ignoring the lyrical content. Emotion extraction from text can be a difficult task, due to the subjectivity in quantifying or discretely categorizing emotions. In this project, 2 of the popular models of emotions, the Plutchik's Wheel and the VAD model, have been used to attempt emotion extraction. The tracks were reclassified using their emotion intensities extracted from their lyrical content.
\end{abstract}

\section{Introduction}
This paper discusses the methods used to extract emotion intensities from lyrics and using them to reclassify randomly selected songs into emotion-specific playlists.

\section{Background}

Most popular streaming services contain manually curated playlists of songs based on their perceived emotions they invoke on the listeners. For example, Spotify has many official mood specific playlists. The apparent moods they invoke range from highly energetic or angry songs to play at the gym to soft or sad songs to play when it is raining. These playlists are constantly updated to incorporate new releases, but are done so manually. This manual curation can be heavily influences from staff bias and subjectivity.

On Spotify, automatic playlist creation is achieved via audio features such as duration, energy, tempo, etc. and other combinations of audio features such as danceability and instrumentalness \cite{spotify}. These features are evaluated from audio analyses of the tracks which do not include contexts from their lyrical contents.

Lyrics can be just as relevant when classifying songs into moods. For example, the production of \textit{Pumped Up Kicks} by the indie pop/neo-psychedelia group Foster the People consists of pop chords, upbeat melodies and a relatively cheerful tone. However, the song describes the homicidal fantasies of a young boy named Richard who has gained access to his father's gun.

\setlength{\fboxrule}{0pt}
\begin{BOX}[ht]
  \centering
  \fbox{\begin{minipage}{\dimexpr\textwidth-2cm}
      \footnotesize
      \centering
      He found a six-shooter gun                         \\
      In his dad's closet, and with a box of fun things  \\
      I don't even know what                             \\
      But he's coming for you, yeah, he's coming for you \\
      All the other kids with the pumped up kicks        \\
      You better run, better run outrun my gun           \\
      All the other kids with the pumped up kicks        \\
      You better run, better run faster than my bullet
    \end{minipage}
  }
  \caption*{\textbf{Listing 1:} Excerpt of \textit{Pumped Up Kicks}}
\end{BOX}

Relying solely on the audio features of this song will classify it as a joyous or calming song, with relatively positive sentiment.

\subsection{Natural Language Processing on Lyrics}

Natural language processing on English song lyrics assume additional restrictions. There are no standards set for preprocessing lyrics, but the following lists the exceptions address for this project:
\begin{itemize}
  \item Songs may be entirely composed of stopwords.
  \item Repetition is considered significant.
  \item Lyrics may contain songwriting directions, such as "[gang vocals]", "[instrumental]", "[hook]", "[Speaker A]", etc.
  \item Lyrics may contain adlibs. For this project, adlibs are not considered part of the dictionary.
\end{itemize}

\subsection{Sentiment Analysis}

Sentiment scores are widely used to categorize documents as invoking positive or negative perceptions. However, the scores are limited as they are one-dimensional, ranging between [-1, 1]. This score lacks the depth required to distinguish negative-sentiment emotions such as anger from sadness or positive-sentiment emotions such as calm and excited.

\subsection{Emotion Classification}

The field of emotion classification is highly subjective. It is often difficult to quantify with absolute values or discretely classify emotions. There have been several psychological models developed over the decades. This project utilizes 2 of the popular models, the Plutchik's Wheel of Emotions and the VAD Emotional State Model.

\subsubsection{Plutchik's Wheel of Emotions Model}

Psychologist Robert Plutchik proposed a model composed of eight primary emotions: anger, fear, sadness, disgust, surprise, anticipation, trust, and joy \cite{wheel}. The model, pictured in Figure \ref{fig:wheel}, consists of several concentric circles, with the outer circles being combinations of emotions from the inner circles.

\begin{figure}[!ht]
  \includegraphics[scale=0.4]{../statics/wheel.png}
  \centering
  \caption{Plutchik's Wheel of Emotions Model}
  \label{fig:wheel}
\end{figure}

Several categories described by the model do not translate well over text. For example, a human reviewer may find it difficult to extract emotions of \textit{trust} or \textit{anticipation} from a document without explicit usages of synonyms of such emotions.

\subsubsection{VAD Emotional State Model}

The VAD (Valence-Arousal-Dominance) Emotional State Model was proposed by psychologist Albert Mehrabian. The model plots emotional states across these 3 dimensions of emotion. Valence measures how pleasant or unpleasant an emotion is, arousal determines the energy of the emotion, and dominance refers to the sense of control over the particular emotion. The model, pictured in \ref{fig:vad}, implies a more granular approach to categorizing emotions.

\begin{figure}[!ht]
  \includegraphics[scale=0.35]{../statics/vad_.png}
  \centering
  \caption{VAD Emotional State Model}
  \label{fig:vad}
\end{figure}

The third dimension of the model can be disregarded to describe the more popular Valence-Arousal Emotional State Model (also known as the Circumplex Model), developed by psychologist James A. Russell. The two dimensions of this model allows for emotions to be categorized into quadrants which are sufficient in determining the general sentiment of the emotion. The four quadrants of the model, pictured in \ref{fig:va}, can be labeled as:
\begin{itemize}
  \item \textbf{Quadrant I}: High-arousal/positive-valence, "joy"
  \item \textbf{Quadrant II}: High-arousal/negative-valence, "anger"
  \item \textbf{Quadrant III}: Low-arousal/negative-valence, "sadness"
  \item \textbf{Quadrant IV}: Low-arousal/positive-valence, "calm"
\end{itemize}

\begin{figure}[!ht]
  \includegraphics[scale=0.45]{../statics/va_.png}
  \centering
  \caption{VA Emotional State Model}
  \label{fig:va}
\end{figure}

\section{Source Datasets}

The datasets for this project were obtained from various sources.

\subsection{Playlist Datasets}

Playlist datasets were obtained from Spotify via their web application programming interface (API) \cite{spotify}. Top playlists curated by the Spotify staff were randomly selected, and the API was used to gather playlist names, lists of tracks, and the apparent targeted moods.

\subsection{Lyrics}

The lyrics of all of the tracks from the playlist datasets were obtained via scraping from various web sources. In total, 748 documents were obtained for analysis. The lyrics are tokenized as 1-grams and therefore lose context. The lyrics are tokenized, stemmed and preprocessed in 3 different ways:
\begin{enumerate}
  \item With the frequencies preserved, represented as bags-of-words
  \item With the frequencies normalized, represented as dictionaries with equal weights
  \item With the frequencies normalized and stopwords removed, represented as dictionaries with equal weights
\end{enumerate}

\subsection{NRC Emotion Lexicons}

The word associations, emotion intensities and the VAD lexicons for the emotion models were obtained from the National Research Council Canada \cite{LREC18-AIL,Mohammad13,vad-acl2018}. The dictionaries for the positive and negative terms required to compute the sentiment scores were also extracted from these datasets.

\clearpage
\newpage
\section{Scores}

The scores generated are based on sums of the emotion intensities and frequencies of word associations.

\subsection{Emotion Intensity}

Emotion intensity scores are generated for the 8 emotions identified by the Plutchik's Wheel Model. The scores are simply the sums of the corresponding emotion intensity values present in the NRC datasets. To normalize the intensity scores, each of the 8 emotions also have their corresponding intensity ratio scores, which are calculated by computing the ratio of the emotion intensities over the total sum of intensities. Equation \ref{equ:emo_ratio} defines the ratio for all emotions $e$ in the 8 emotions identified by the Plutchik's Wheel Model, $E_{wheel}$.

\begin{equation} \label{equ:emo_ratio}
  e_{ratio} = \frac{\sum_{i=0}e_i}{\sum_{e \in E_{wheel}}\sum_{i=0}e_i}
\end{equation}

\subsection{VAD Scores}

Similar to the emotion intensity scores, the valence, arousal and dominance scores were evaluated by summing their intensities and averaging them over the number of terms identified in the word associations. Since every term in the dataset are associated with a 3-tuple of valence, arousal and dominance values, their sums are averaged by the same value. Equation \ref{equ:vad} defines the ratios.

\begin{equation} \label{equ:vad}
  v = \frac{\sum_{i=0}^{N}v_i}{N}, a = \frac{\sum_{i=0}^{N}a_i}{N}, d = \frac{\sum_{i=0}^{N}d_i}{N}
\end{equation}

\subsection{Sentiment Scores}

The sentiment scores are instead evaluated as the ratio of the positive terms ($p$) and negative terms ($n$) over the total number of terms. Terms are labeled positive or negative if they are found in their corresponding datasets.

\begin{equation}
  S = \frac{|p| - |n|}{N}
\end{equation}

If a document contains more negative terms, then the sentiment score will naturally be negative as well, and vice versa for a positive score.

\section{Exploratory Analysis}

\subsection{Emotion Intensities}

Table \ref{table:transform_dist} show the mean of the distributions of the emotion intensities over the different types of normalization. The distributions of the intensities over the unnormalized lyrics and the normalized lyrics with the stopwords removed appear to be similar.

\begin{simptable}
  {Means of Emotion Intensities Over Different Normalization Processes}
  {transform_dist}
  {|c|R{0.18\linewidth}|R{0.18\linewidth}|R{0.18\linewidth}|}
  \textbf{Emotion} & \textbf{No normalization} & \textbf{Frequency normalization} & \textbf{Frequency normalization with stopping} \\
  \hline
  anger        & 1.436 & 3.609 & 1.888 \\
  \hline
  anticipation & 1.488 & 3.320 & 1.002 \\
  \hline
  disgust      & 0.950 & 2.336 & 1.354 \\
  \hline
  fear         & 1.626 & 3.437 & 1.825 \\
  \hline
  joy          & 2.131 & 5.796 & 2.876 \\
  \hline
  sadness      & 1.567 & 3.489 & 1.486 \\
  \hline
  surprise     & 0.717 & 1.559 & 0.624 \\
  \hline
  trust        & 2.145 & 6.043 & 2.199 \\
  \hline
\end{simptable}

Table \ref{table:ratio_dist} shows the distributions of the 8 emotion ratios.

\begin{simptable}
  {Distribution of Emotion Ratios}
  {ratio_dist}
  {|c|c|c|c|}
  \textbf{Emotion} & \textbf{mean} & \textbf{median} & \textbf{max} \\
  \hline
  anger\_ratio        &  0.110 &  0.106 &  0.676 \\
  \hline
  anticipation\_ratio &  0.147 &  0.132 &  0.624 \\
  \hline
  disgust\_ratio      &  0.073 &  0.064 &  0.287 \\
  \hline
  fear\_ratio         &  0.131 &  0.123 &  0.506 \\
  \hline
  joy\_ratio          &  0.226 &  0.204 &  0.806 \\
  \hline
  sadness\_ratio      &  0.139 &  0.131 &  0.687 \\
  \hline
  surprise\_ratio     &  0.064 &  0.060 &  0.256 \\
  \hline
  trust\_ratio        &  0.221 &  0.207 &  0.706 \\
  \hline
\end{simptable}

The aforementioned song, \textit{Pumped Up Kicks}, was also evaluated for its emotion ratios. Table \ref{table:foster} shows the prevalent emotions found from the lyrics of the song.
\begin{simptable}
  {Prevelant Emotions Extracted from \textit{Pumped Up Kicks}}
  {foster}
  {|c|c|c|}
  \textbf{Emotion Ratio} & \textbf{Score} \\
  \hline
  fear  & 0.378 \\
  \hline
  anger  & 0.306 \\
  \hline
\end{simptable}

\subsection{Valence, Arousal, and Dominance}

All of the tracks had their valence, arousal, and dominance scores evaluated and displayed over the scatter plot in Figure \ref{fig:vad_pl}.

\begin{figure}[!ht]
  \includegraphics[scale=0.3]{../statics/plots/vad.png}
  \centering
  \caption{Valence, Arousal, and Dominance of All Playlists}
  \label{fig:vad_pl}
\end{figure}
The data points appear to cluster on the valence-arousal plane, with the exception of some outliers in the dominance axis. When translated into a valence-arousal graph, pictured in Figure \ref{fig:va_pl}, the data points appear to cluster in Quadrant IV. There does not appear to be any correlations among the playlists.
\begin{figure}[!ht]
  \includegraphics[scale=0.35]{../statics/plots/va_playlist_False.png}
  \centering
  \caption{Valence and Arousal of All Playlists}
  \label{fig:va_pl}
\end{figure}

\clearpage
\newpage
\section{Generated Playlist}

The emotion ratio values were used to classify the tracks into generated playlists, Emotion Playlist and Quadrant Playlist.

\subsection{Emotion Playlist}

Emotion Playlists were generated by grouping tracks if the corresponding emotion ratios exceeded a threshold. The threshold was determined by evaluating a distribution measure, such as the median or mean. The best distribution measure was determined by comparing the accuracies. Equation \ref{equ:accuracy} defines the formula used to calculate accuracy.
\begin{equation} \label{equ:accuracy}
  acc = \frac{\sum_{e^{+} \in E^{+}}\frac{\sum_{i}[e^{+}_i > 0]}{|e^{+}|} + \sum_{e^{-} \in E^{-}}\frac{\sum_{i}[e^{-}_i < 0]}{|e^{-}|}}{|E|}
\end{equation}

Table \ref{table:wheel_accuracy} shows the accuracies and losses evaluated with the different distribution measures. The loss (Equation \ref{equ:loss}) is calculated as the ratio of uncategorized tracks over the total number of tracks.

\begin{equation} \label{equ:loss}
  loss = \frac{t_{uncat}}{t_{cat}+t_{uncat}}
\end{equation}

\begin{simptable}
  {Accuracies of Emotion Playlist with Different Measures}
  {wheel_accuracy}
  {|c|c|c|}
  \textbf{Metric} & \textbf{Loss} & \textbf{Accuracy} \\
  \hline
  mean  & 0.13\% & 0.673 \\
  \hline
  25\% quantile  & 0.13\% & 0.544 \\
  \hline
  50\% quantile  & 0.13\% & 0.643 \\
  \hline
  75\% quantile  & 2.27\% & 0.748 \\
  \hline
\end{simptable}

The best accuracy was achieved with the 75\% quantile. The tracks were reclassified into the corresponding Emotion Playlists using this measure. Their valence-arousal scores were plotted over the scatter plot pictured in Figure \ref{fig:va_wheel}.

\begin{figure}[!ht]
  \includegraphics[scale=0.35]{../statics/plots/va_wheel_playlist_False.png}
  \centering
  \caption{Valence and Arousal of Emotion Playlists}
  \label{fig:va_wheel}
\end{figure}

Some clustering can be observed with the Emotion Playlists. However, they do not seem to be discrete enough to be considered concrete.

The distribution of the emotion ratios were plotted against the Emotion Playlists, pictured in Figure \ref{fig:wheel_playlist}

\begin{figure}[!ht]
  \includegraphics[scale=0.42]{../statics/plots/wheel_playlist.png}
  \centering
  \caption{Distribution of Sentiment Scores Over The 8 Emotions}
  \label{fig:wheel_playlist}
\end{figure}

% \newpage
\subsection{Quadrant Playlist}

The Quadrant Playlists were generated by grouping pairs of Plutchik's 8 emotions into the quadrants of the valence-arousal graph. The following emotion quadrants were paired:
\begin{itemize}
  \item \textbf{Quadrant I}: "joy" and "surprise"
  \item \textbf{Quadrant II}: "anger" and "disgust"
  \item \textbf{Quadrant III}: "sadness" and "fear"
  \item \textbf{Quadrant IV}: "trust" and "anticipation"
\end{itemize}

With these combinations, a track's quadrant scores were calculated using Equation \ref{equ:quad_score}.

\begin{equation} \label{equ:quad_score}
  e_{q} = \frac{e_{1,ratio}+e_{2,ratio}}{2}
\end{equation}

Similar to the Emotion Playlists, the tracks were reclassified into Quadrant Playlists if their quadrant scores exceeded a threshold determined by the best accuracy with different distribution measures. The accuracy was calculated using Equation \ref{equ:quad_accuracy}.

\begin{equation} \label{equ:quad_accuracy}
  acc = \frac{\sum_{q^{+} \in Q^{+}}\frac{\sum_{i}[q^{+}_i > 0]}{|q^{+}|} + \sum_{q^{-} \in Q^{-}}\frac{\sum_{i}[q^{-}_i < 0]}{|q^{-}|}}{|Q|}
\end{equation}

Table \ref{table:quad_accuracy} shows the accuracies and losses evaluated with the different distribution measures.

\begin{simptable}
  {Accuracies of Quadrant Playlist with Different Measures}
  {quad_accuracy}
  {|c|c|c|}
  \textbf{Metric} & \textbf{Loss} & \textbf{Accuracy} \\
  \hline
  mean  & 14.44\% & 0.713 \\
  \hline
  25\% quantile  & 16.31\% & 0.572 \\
  \hline
  50\% quantile  & 14.44\% & 0.719 \\
  \hline
  75\% quantile  & 18.45\% & 0.820 \\
  \hline
\end{simptable}

The best accuracy was achieved with the 75\% quantile. The tracks were reclassified into the corresponding Quadrant Playlists using this measure. The distribution of the emotion ratios were plotted against the Emotion Playlists, pictured in Figure \ref{fig:quad_playlist}.

\begin{figure}[!ht]
  \includegraphics[scale=0.42]{../statics/plots/quadrant_playlist.png}
  \centering
  \caption{Distribution of Sentiment Scores Over The 4 Quadrants}
  \label{fig:quad_playlist}
\end{figure}

\section{Conclusion}

The tracks retrieved from Spotify playlists were automatically reclassified using emotions extracted from their lyrics. Using the emotion intensities, valence-arousal scores and sentiment scores, new playlists based on their prevalent emotions were created with decent sentiment score accuracy. With more tracks, it would be possible to increase the accuracy and even employ the use of machine learning classifiers.

% \clearpage
\addcontentsline{toc}{section}{References}
\bibliographystyle{ieeetr}
\bibliography{refs}

\end{document}
