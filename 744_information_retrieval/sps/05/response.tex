\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{array}
\usepackage{caption}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{listings}

\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\definecolor{Gray}{gray}{0.85}
\newcolumntype{a}{>{\columncolor{Gray}}M}

% margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{605.744: Information Retrieval \\ Problem Set (Module 5)}
\author{Sabbir Ahmed}
\date{\today}

\begin{document}
\maketitle

    \begin{enumerate}

        \item (10\%) Briefly describe the three key assumptions of the Cranfield paradigm for information retrieval evaluation.

        \textbf{Answer:}

        \item (10\%) What is pooling and why is it used in large-scale text retrieval evaluations?

        \textbf{Answer:}
        % Given information needs and documents, you need to collect relevance assessments. This is a time-consuming and expensive process involving human beings. For tiny collections like Cranfield, exhaustive judgments of relevance for each query and document pair were obtained. For large modern collections, it is usual for relevance to be assessed only for a subset of the documents for each query. The most standard approach POOLING is pooling, where relevance is assessed over a subset of the collection that is formed from the top k documents returned by a number of different IR systems (usually the ones to be evaluated), and perhaps other sources such as the results of Boolean keyword searches or documents found by expert searchers in an interactive process.        

        \item (50\%) Consider a query with 10 relevant documents whose docids are: D3, D27, D30, D39, D51, D54, D69, D72, D81, and D96. Assume that all other documents are not relevant. On this query two retrieval systems \textit{FastSearch} and \textit{Telescope} produce the following ranked lists. (Note: D17 is the 1st ranked doc by \textit{FastSearch}; D4 is its 2nd ranked doc, etc ...)

        \textit{FastSearch}: D17, D4, D69, D54, D37, D41, D89, D85, D3, D5, D91, D39

        \textit{Telescope}: D3, D1, D94, D27, D50, D54, D16, D7, D72, D39, D95, D62

        \begin{enumerate}

            \item How many relevant documents are found by each system?

            \textbf{Answer:}

            \item For both systems what is P@10 (precision at 10 documents) for this query?

            \textbf{Answer:}

            \item For \textit{FastSearch} what is the uninterpolated precision at 30\% Recall?

            \textbf{Answer:}

            \item Assuming that \textit{FastSearch} returns no other documents other than this top-12 ranked list, what is \textit{FastSearch}'s Recall for this query?

            \textbf{Answer:}

            \item For both systems what is average precision on this query?

            \textbf{Answer:}

        \end{enumerate}

        \item (15\%) Given two retrieval systems (called A and B), is it possible for System A to be better than System B in
        average precision, but for System B to have higher P@10 than System A? Briefly justify your response.

        \textbf{Answer:}

        \item (15\%) Consider the contingency tables below for the word pairs (bicycle, helmet) and (bicycle, repairs). Suppose we are looking to expand a query containing the word bicycle by adding some potentially useful search terms. Using pointwise mutual information (PMI) to score candidate terms, calculate scores for both helmet and repairs, and indicate which of the two would be the better expansion term. N = 15,000 documents. Use base 2 logs.

        \begin{equation*}
            PMI(x,y)=log_2\left(\frac{P(x,y)}{P(x)P(y)}\right)=log_2\left(\frac{N\times a}{(a+b)(a+c)}\right)
        \end{equation*}

        \begin{table}[ht]
            \footnotesize
            \begin{tabular}[t]{|a{1.6cm}|a{1.6cm}|}
                \hline
                A: docs with both terms together & B: docs with first term, but not second
                \\ \hline
                C: docs with second term, but not first & D: docs that contain neither term
                \\ \hline
            \end{tabular}
            \hfill
            \begin{tabular}[t]{|M{1.15cm}|M{1.15cm}|M{1.15cm}|}
                \hline
                & has helmet & missing helmet
                \\ \hline
                has bicycle & 22 & 54
                \\ \hline
                missing bicycle & 87 & 14837
                \\ \hline
            \end{tabular}
            \hfill
            \begin{tabular}[t]{|M{1.15cm}|M{1.15cm}|M{1.15cm}|}
                \hline
                & has helmet & missing helmet
                \\ \hline
                has bicycle & 31 & 45
                \\ \hline
                missing bicycle & 164 & 14760
                \\ \hline
            \end{tabular}
        \end{table}

        \textbf{Answer:}

    \end{enumerate}

\end{document}
