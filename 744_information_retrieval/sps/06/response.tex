\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{array}
\usepackage{caption}
\usepackage{colortbl}
\usepackage{graphicx}
\usepackage{listings}

\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}
\definecolor{Gray}{gray}{0.85}
\newcolumntype{a}{>{\columncolor{Gray}}M}

% margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{605.744: Information Retrieval \\ Problem Set (Module 5)}
\author{Sabbir Ahmed}
\date{\today}

\begin{document}
\maketitle

    \begin{enumerate}

        \item (10\%) Briefly describe the three key assumptions of the Cranfield paradigm for information retrieval evaluation.

        \textbf{Answer:} Relevance judgments may vary due to different systems with different pools where some systems or variants may not have even contributed to pools. However, these potential errors may not be significant if the pool size is sufficient, the collection is of a fixed size, and the evaluation is done to score relative rankings as opposed to absolute measures of performance.

        \item (10\%) What is pooling and why is it used in large-scale text retrieval evaluations?

        \textbf{Answer:} Pooling is a method used to assess the relevance of documents where only a subset of the corpus are considered for review. The subset is created by gathering the top $k$ ranked documents by a number of different IR systems. This method of evaluation is usually performed on large corpuses where evaluating the number of documents may become infeasible.

        \item (50\%) Consider a query with 10 relevant documents whose docids are: 
        D3, D27, D30, D39, D51, D54, D69, D72, D81, and D96. Assume that all other documents are not relevant. On this query two retrieval systems \textit{FastSearch} and \textit{Telescope} produce the following ranked lists. (Note: D17 is the 1st ranked doc by \textit{FastSearch}; D4 is its 2nd ranked doc, etc ...)

        \textit{FastSearch}: D17, D4, D69, D54, D37, D41, D89, D85, D3, D5, D91, D39

        \textit{Telescope}: D3, D1, D94, D27, D50, D54, D16, D7, D72, D39, D95, D62

        \begin{enumerate}

            \item How many relevant documents are found by each system?

            \textbf{Answer:} \textit{FastSearch} retrieved 4 relevant documents (\{D69, D54, D3, D39\}) and \textit{Telescope} retrieved 5 relevant documents (\{D3, D27, D54, D72, D39\}).

            \item For both systems what is P@10 (precision at 10 documents) for this query?

            \textbf{Answer:} P@10 can be computed by the expression $\frac{r}{10}$, where $r$ is the number of relevant documents retrieved at 10. \textit{FastSearch} scores 0.3 with \{D69, D54, D3\} while \textit{Telescope} scores 0.5 on this metric.

            \item For \textit{FastSearch} what is the uninterpolated precision at 30\% Recall?

            \textbf{Answer:} At 30\% recall in \textit{FastSearch}, the retrieved documents are \{D17, D4, D69, D54, D37, D41, D89, D85, D3\}, which is 9 documents. This scores the precision to $\frac{9}{12}$ or 75\%.

            \item Assuming that \textit{FastSearch} returns no other documents other than this top-12 ranked list, what is \textit{FastSearch}'s Recall for this query?

            \textbf{Answer:} Recall is computed with the expression: $\frac{A}{A+C}$ where $A$ is the number of retrieved relevant documents and $C$ is the number of relevant documents not retrieved. \textit{FastSearch} retrieved 4 relevant documents and did not retrieve the other 6 documents, which scores its recall to $\frac{4}{10}$ or 0.4.

            \item For both systems what is average precision on this query?

            \textbf{Answer:} The average precision is computed by summing the precisions at the retrieved relevant documents in the system. For \textit{FastSearch} the retrieved documents are ranked \{3, 4, 9, 12\} with precisions of \{1/3=0.33, 2/4=0.5, 3/9=0.33, 4/12=0.25\}. This totals to 1.41 for \textit{FastSearch}.

            For \textit{Telescope}, the retrieved documents are ranked \{1,4,6,9,10\} with precisions of \{1/1=1, 2/4=0.5, 3/6=0.5, 4/9=0.44, 5/10=0.5\} which totals to 2.94.

        \end{enumerate}

        \item (15\%) Given two retrieval systems (called A and B), is it possible for System A to be better than System B in average precision, but for System B to have higher P@10 than System A? Briefly justify your response.

        \textbf{Answer:} Yes, it is possible for a system to have a higher P@10 even with a lower average precision. For example, System B may have retrieved 5 relevant documents in its top 10 ranks where System A retrieved 4, setting the P@10 scores to $0.5$ and $0.4$ respectively.

        For simplicity, we can let all the relevant documents retrieved by System B be placed on even ranks until the top 10 ranks. This placement will attribute an average precision of System B to $0.5\times 5=2.5$. If we let the 4 relevant documents retrieved by System A be placed on its top 4 ranks, then the average precision will total to $1\times 4=4$.

        \item (15\%) Consider the contingency tables below for the word pairs (bicycle, helmet) and (bicycle, repairs). Suppose we are looking to expand a query containing the word bicycle by adding some potentially useful search terms. Using pointwise mutual information (PMI) to score candidate terms, calculate scores for both helmet and repairs, and indicate which of the two would be the better expansion term. N = 15,000 documents. Use base 2 logs.

        \begin{equation*}
            PMI(x,y)=log_2\left(\frac{P(x,y)}{P(x)P(y)}\right)=log_2\left(\frac{N\times a}{(a+b)(a+c)}\right)
        \end{equation*}

        \begin{table}[ht]
            \footnotesize
            \begin{tabular}[t]{|a{1.6cm}|a{1.6cm}|}
                \hline
                A: docs with both terms together & B: docs with first term, but not second
                \\ \hline
                C: docs with second term, but not first & D: docs that contain neither term
                \\ \hline
            \end{tabular}
            \hfill
            \begin{tabular}[t]{|M{1.15cm}|M{1.15cm}|M{1.15cm}|}
                \hline
                & has helmet & missing helmet
                \\ \hline
                has bicycle & 22 & 54
                \\ \hline
                missing bicycle & 87 & 14837
                \\ \hline
            \end{tabular}
            \hfill
            \begin{tabular}[t]{|M{1.15cm}|M{1.15cm}|M{1.15cm}|}
                \hline
                & has repairs & missing repairs
                \\ \hline
                has bicycle & 31 & 45
                \\ \hline
                missing bicycle & 164 & 14760
                \\ \hline
            \end{tabular}
        \end{table}

        \textbf{Answer:} The PMI for the first table is:
        \begin{align*}
            log_2\left(\frac{N\times a}{(a+b)(a+c)}\right) &= log_2\left(\frac{15000\times 22}{(22+54)(22+87)}\right) \\
            &= log_2\left(\frac{82500}{2071}\right) \\
            &= 5.316
        \end{align*}

        \textbf{Answer:} The PMI for the second table is:
        \begin{align*}
            log_2\left(\frac{N\times a}{(a+b)(a+c)}\right) &= log_2\left(\frac{15000\times 31}{(31+45)(31+164)}\right) \\
            &= log_2\left(\frac{7750}{247}\right) \\
            &= 4.972
        \end{align*}

        Since the PMI for the contingency table of the word pairs (bicycle, helmet) is higher, "helmet" should be considered as the better expansion term.

    \end{enumerate}

\end{document}
