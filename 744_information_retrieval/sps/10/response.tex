\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{textcomp}

\usepackage[autostyle, english=american]{csquotes}
\MakeOuterQuote{"}
\captionsetup[table]{skip=1pt}

\newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

% margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\title{605.744: Information Retrieval \\ Problem Set (Module 11)}
\author{Sabbir Ahmed}
\date{\today}

\begin{document}
\maketitle

\begin{enumerate}

      \item (30\%) \textbf{Question Typology.} IBM's Watson team developed over a hundred ways to answer a Jeopardy! question. A key part of question answering is determining what kind of question is being asked. Mark each question below with one of the following question categories: QUANTITY; WHERE; WHEN; WHO; WHAT-SUBTYPE; OTHER. Finally, write a regular expressions (or create a pattern like a regular expression) that correctly identifies all of the QUANTITY questions below but does not mis-identify any of the other questions as QUANTITY.

            \begin{enumerate}
                  \item At what temperature does water freeze?

                        \textbf{Answer:} QUANTITY

                  \item How tall is the Eiffel tower in Lisbon?

                        \textbf{Answer:} QUANTITY

                  \item Who killed Bobby Kennedy?

                        \textbf{Answer:} WHO

                  \item How far is the Earth from the Sun?

                        \textbf{Answer:} QUANTITY

                  \item What flying mammal navigates using echolocation?

                        \textbf{Answer:} WHO

                  \item Where did Elena Kagan attend law school?

                        \textbf{Answer:} WHERE

                  \item What US federal agency is responsible for collecting tariffs?

                        \textbf{Answer:} WHAT-SUBTYPE

                  \item Who invented Post-It notes?

                        \textbf{Answer:} WHO

                  \item How do you change the oil in a Ford Explorer?

                        \textbf{Answer:} HOW

                  \item When did Idaho become a state?

                        \textbf{Answer:} WHEN

                  \item Which national monument did Dr. Martin Luther King Jr. give a famous speech at?

                        \textbf{Answer:} WHERE

                  \item How tall is the belfry in Bruges?

                        \textbf{Answer:} QUANTITY

                  \item What city was Angela Merkel born in?

                        \textbf{Answer:} WHERE

                  \item How many feet are there in a nautical mile?

                        \textbf{Answer:} QUANTITY

                  \item What is the state flower of Maryland?

                        \textbf{Answer:} WHAT-SUBTYPE

            \end{enumerate}

            \textbf{Answer:} The regular expression would be: \texttt{(How\textbackslash s(?!do))|(What\textbackslash stemperature)}.

      \item (25\%) \textbf{Named Entity Recognition (NER).} A NER tool tries to identify named entities in text (i.e., persons, organizations, locations). Examine the online Stanford NER tool available at: https://corenlp.run. Enter a variety of text and examine the results.
            \begin{enumerate}
                  \item Give an example of an input sentence and error that you think the tool should not make.

                        \textbf{Answer:} \textit{I can't wait to try cooking a turkey this Thanksgiving!}

                  \item What types of errors are you able to discover?

                        \textbf{Answer:} The name entity \textit{Thanksgiving} was properly recognized as a \texttt{DATE} entity, but \textit{turkey} gets improperly categorized as a \texttt{COUNTRY} entity.

                  \item Now suppose that you had a perfect named-entity recognizer (i.e., one that makes no mistakes). Argue briefly and clearly whether or not this capability could be used to effectively enhance ad hoc text retrieval accuracy (i.e., as measured by say average precision). Explain your reasoning and give examples if helpful.

                        \textbf{Answer:}

            \end{enumerate}

      \item (15\%) \textbf{Retrieving with Good Sense.} Read Mark Sanderson's paper "Retrieving with Good Sense" (this is an assigned reading). In a few sentences briefly describe Sanderson's kalishnikov/banana experiment. You should explain what the goal of the experiment was and what was learned.

            \textbf{Answer:} Sanderson attempted to artificially add ambiguity to a corpus by joining random words together to create pseudowords. The constituent words are then replaced by the pseudoword, i.e. all occurrences of "kalishnikov" and "banana" in the corpus get replaced by "kalishnikov/banana". Sanderson used this approach to add $n$-sized pseudowords (pseudowords made of $n$ distinct words) and determine the effectiveness of an IR system. It was found that adding pseudowords did not reduce effectiveness as much as might have been expected.

      \item (15\%) \textbf{Lexical Semantic Relations.} Make up your own examples for the following lexical semantic relations. Example: "Two words that share an antonymy relation" - excited / calm

            \begin{itemize}
                  \item Two words that share a hypernym / hyponym relation:

                        \textbf{Answer:} color / red

                  \item Two words that share a demonym relation:

                        \textbf{Answer:} Spain / Spanish

                  \item Two words that share a synonymy relation:

                        \textbf{Answer:} copy / duplicate

                  \item Two words that share a meronymy relation:

                        \textbf{Answer:} book / library

                  \item Two words that share a troponymy relation:

                        \textbf{Answer:} laugh / giggle

            \end{itemize}

      \item (15\%) \textbf{Using WordNet.} Explore the on-line version of WordNet, which can be found at http://wordnetweb.princeton.edu/perl/webwn. Lookup the detailed entries for these words: alphabet, delta, oracle, yeti.

            Given what you observe by looking up these words, what conclusions can you make about using dictionary-based word-sense disambiguation (e.g., using a resource like WordNet) to try and improve text retrieval performance?

            \textbf{Answer:}


\end{enumerate}

\end{document}
