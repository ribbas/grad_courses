# Question 1

The discussion under this topic will take a few weeks, (giving us a bit of a break) and will take on a couple of flavors, covering semaphores and message queues as well, which will be exposed when those modules are released. I'll post into the discussion area with direction at the appropriate time.
As in previous modules, feel free to post to the discussion any answers to the questions posed in the course content. You can also talk about how you might implement linked lists in shared memory (this is essentially question 15.13 in the Exercises of Chapter 15 of your text). (But only one student can get credit for the right answer - the student who posted it first).
In addition, we can look a bit more deeply at the manner in which shared memory (and the other System V IPC mechanisms) were developed. In the overview, we saw how the System V IPC facilities are not quite like the UNIX facilities we have looked at to date. First off, summarize a point that makes these facilities differ from the "UNIX philosophy" (what is that?). How would you do it differently, in a way more attuned to the UNIX philosophy? Also, what problems do you need to look out for when using these facilities due to these issues? You can look at this with regard to shared memory, first - but as we move on, you can add comments about semaphores and/or message queues as well.

# Answer 1

The shortcomings of SysV IPCs regarding the UNIX Philosophy appear to have been detailed very well in the other posts. One specific limitation I wanted to focus on was their inability to automatically reallocate resources after use. This was such an issue in a project at my work that it lead to an unnecessarily lengthy testing phase.

I had to research methods to reduce the latency of communication between interoperating processes on a single node. The processes were spawned in cluster networks, so optimizations on singular node levels were highly desired. The IPC that was initially chosen was UNIX domain sockets [1] which do not require processes to free up resources after use. This can be advantageous in a program where multiple processes are continuously spawned and required to communicate with each other. Having the kernel automatically reallocate resources for use again saves a decent workload for the individual processes. However, shared memories, even with their restrictions on having to manually free resources, were also seriously considered due to the advantage they have in speed. The requirement for a massive increase in overhead if the project was to use shared memories was instantly recognized, but the trade-off with the speed being magnitudes better than sockets could not be ignored. Another advantage that was observed was that shared memories can guarantee not to block, which can be beneficial to multiple processes attempting to synchronize on large scale. Shared memories ended up as an optional method of communication in the project for users who preferred the speed gain even if it came with the complexity.

This demonstrated that even though a SysV IPC can be against all the rules detailed in the UNIX philosophy, they have enough advantages on compatible systems to gain supporters.

[1]: https://www.ibm.com/docs/en/ztpf/1.1.0.15?topic=considerations-unix-domain-sockets
