{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 605.629: Programming Languages\n",
    "## Assignment 3\n",
    "\n",
    "Sabbir Ahmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \\[100 pts, top down parser\\]\n",
    "\n",
    "Utilizing cells from the Jupyter notebook provided in lecture modules, write a top-down parser than can parse the following:\n",
    "\n",
    "```\n",
    "alist = [1, 3, 4]\n",
    "sum = 0\n",
    "for a in alist:\n",
    "    sum = sum + a\n",
    "```\n",
    "\n",
    "Your solution might include only the necessary cells and the output should be at least similar to the following:\n",
    "\n",
    "```\n",
    "(= (name alist) ([ [(literal 1), (literal 3), (literal 4)]))\n",
    "(= (name sum) (literal 0))\n",
    "(for [(name a)] (name alist))\n",
    "(= (name sum) (+ (name sum) (name a)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following snippet has been extracted from various cells in the lecture modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import tokenize as tok\n",
    "import re\n",
    "\n",
    "\n",
    "# Update the regular expression pattern to find two character matches with '**'\n",
    "token_pattern = re.compile(r\"\\s*(?:(\\d+)|(\\*\\*|.))\")\n",
    "\n",
    "\n",
    "class literal_token:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def nud(self):\n",
    "        return self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(literal %s)\" % self.value\n",
    "\n",
    "\n",
    "class end_token:\n",
    "    lbp = 0\n",
    "\n",
    "\n",
    "def tokenize_python(_input_program):\n",
    "    type_map = {\n",
    "        tok.NUMBER: \"(literal)\",\n",
    "        tok.STRING: \"(literal)\",\n",
    "        tok.OP: \"(operator)\",\n",
    "        tok.NAME: \"(name)\",\n",
    "    }\n",
    "    for t in tok.generate_tokens(io.StringIO(_input_program).readline):\n",
    "        try:\n",
    "            yield type_map[t[0]], t[1]\n",
    "        except KeyError:  # Handling other token values\n",
    "            if t[0] == tok.ENDMARKER or t[0] == tok.NEWLINE:\n",
    "                break\n",
    "            else:\n",
    "                raise SyntaxError(\"Syntax error\")\n",
    "    yield \"(end)\", \"(end)\"\n",
    "\n",
    "\n",
    "def tokenize(_input_program):\n",
    "    for id, value in tokenize_python(_input_program):\n",
    "        if id == \"(literal)\":\n",
    "            symbol = symbol_table[id]\n",
    "            s = symbol()\n",
    "            s.value = value\n",
    "        else:\n",
    "            # name or operator\n",
    "            symbol = symbol_table.get(value)\n",
    "            if symbol:\n",
    "                s = symbol()\n",
    "            elif id == \"(name)\":\n",
    "                symbol = symbol_table[id]\n",
    "                s = symbol()\n",
    "                s.value = value\n",
    "            else:\n",
    "                raise SyntaxError(\"Unknown operator (%r)\" % id)\n",
    "        yield s\n",
    "\n",
    "\n",
    "# advance helper function checks the current token and advances if it has no value, i.e. it is not an expression\n",
    "def advance(id=None):\n",
    "    global token\n",
    "    if id and token.id != id:\n",
    "        raise SyntaxError(\"Expected %r\" % id)\n",
    "    token = next()\n",
    "\n",
    "\n",
    "# helper method to be used as a decorator\n",
    "def method(s):\n",
    "    assert issubclass(s, symbol_base)\n",
    "\n",
    "    def bind(fn):\n",
    "        setattr(s, fn.__name__, fn)\n",
    "\n",
    "    return bind\n",
    "\n",
    "\n",
    "def expression(rbp=0):  # default right binding power set to 0\n",
    "    global token, next  # next shadows the reserved word next\n",
    "    t = token\n",
    "    token = next()  # get the next token from the tokenizer iterator\n",
    "    left = t.nud()  # recursion\n",
    "    while rbp < token.lbp:\n",
    "        t = token\n",
    "        token = next()\n",
    "        left = t.led(left)  # recursion\n",
    "    return left\n",
    "\n",
    "\n",
    "def parse(_program):\n",
    "    global token, next  # the next reserved word is shadowed\n",
    "    next = tokenize(\n",
    "        _program\n",
    "    ).__next__  # tokenizer is an iterator, save its next() function as global\n",
    "    token = next()  # get the next token\n",
    "    return expression()  # parse the expression\n",
    "\n",
    "\n",
    "class symbol_base(object):  # new-style class, derived from Python object class\n",
    "    id = None  # node/token type name\n",
    "    value = None  # used by literals\n",
    "    first = second = third = None  # used by tree nodes\n",
    "\n",
    "    def nud(self):\n",
    "        raise SyntaxError(\"Syntax error (%r).\" % self.id)\n",
    "\n",
    "    def led(self, left):\n",
    "        raise SyntaxError(\"Unknown operator (%r).\" % self.id)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.id == \"(name)\" or self.id == \"(literal)\":\n",
    "            return \"(%s %s)\" % (self.id[1:-1], self.value)\n",
    "        # positions of the operands and operator, left to right\n",
    "        out = [self.id, self.first, self.second, self.third]\n",
    "        # apply the str function to list of out, filter creates the list by applying None\n",
    "        out = map(str, filter(None, out))\n",
    "        return \"(\" + \" \".join(out) + \")\"  # join concatenates the list with the ' '\n",
    "\n",
    "\n",
    "symbol_table = {}\n",
    "\n",
    "\n",
    "def symbol(id, bp=0):\n",
    "    try:\n",
    "        s = symbol_table[id]\n",
    "    except KeyError:\n",
    "\n",
    "        class s(symbol_base):\n",
    "            pass\n",
    "\n",
    "        s.__name__ = \"symbol-\" + id  # for debugging purposes\n",
    "        s.id = id\n",
    "        s.lbp = bp\n",
    "        symbol_table[id] = s\n",
    "    else:\n",
    "        s.lbp = max(bp, s.lbp)\n",
    "    return s\n",
    "\n",
    "\n",
    "def infix(id, bp):\n",
    "    def led(self, left):\n",
    "        self.first = left\n",
    "        self.second = expression(bp)  # recursion\n",
    "        return self\n",
    "\n",
    "    symbol(id, bp).led = led"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines the operator symbols and tokens required for the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol('(literal)').nud = lambda self: self\n",
    "symbol('(name)').nud = lambda self: self\n",
    "symbol('(end)');\n",
    "\n",
    "infix('+', 110)  # Python addition operator\n",
    "infix('=', 60)  # Python assignment operator\n",
    "infix('in', 60)  # Python in operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell implements the symbols required to parse a `list` in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol(',')  # Python container delimiter\n",
    "symbol(']')  # right bracket to enclose Python lists\n",
    "\n",
    "# left bracket to enclose Python lists\n",
    "@method(symbol('['))\n",
    "def nud(self):\n",
    "    self.first = []\n",
    "    if token.id != ']':\n",
    "        while 1:\n",
    "            if token.id == ']':\n",
    "                break\n",
    "            self.first.append(expression())\n",
    "            if token.id != ',':\n",
    "                break\n",
    "            advance(',')\n",
    "    advance(']')\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell implements the symbols required to parse a `for`-loop statement in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol(':')\n",
    "\n",
    "@method(symbol('for'))\n",
    "def nud(self):\n",
    "    self.first = []\n",
    "    if token.id != 'in':\n",
    "        argument_list(self.first)\n",
    "    advance('in')\n",
    "    self.second = expression()\n",
    "    advance(':')\n",
    "    return self\n",
    "\n",
    "def argument_list(list):\n",
    "    while 1:\n",
    "        if token.id != '(name)':\n",
    "            SyntaxError('Expected an argument name.')\n",
    "        list.append(token)\n",
    "        advance()\n",
    "        if token.id != ':':\n",
    "            break\n",
    "        advance(':')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(= (name alist) ([ [(literal 1), (literal 3), (literal 4)]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(= (name sum) (literal 0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(for [(name a)] (name alist))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(= (name sum) (+ (name sum) (name a)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parse(\"alist = [1, 3, 4]\"))\n",
    "display(parse(\"sum = 0\"))\n",
    "display(parse(\"for a in alist:\"))\n",
    "display(parse(\"sum = sum + a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. \\[50 pts bonus\\]\n",
    "\n",
    "Comment about how would you modify your parser program to make \"+=\" work and comment about adding functionality which checks the Python indentation syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the `+=` operator work (addition assignment) we have to first add the symbol to the parser. This can be achieved similarly to how the other tokens were added, with the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "infix('+=', 60)  # Python addition assignment operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(+= (name sum) (name a))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(parse(\"sum += a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the operator to the symbols only allows the parser to accept the token and not throw a syntax error. For the expression to evaluate, it would have to use the addition expression with the 2 operands (`sum` and `a`) and then pass the result into the assignment expression with the 2 operands (`sum + a` and `sum`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding whitespace indentation check to the parser would require some additional constraints to the token patterns. The parser would have to first support scoping. Adding indentation is a syntax error if the expression does not have any preceding lines without any indentation. Indentation may only be accepted after `for/while` loops, `if/else` conditionals, function/class definitions, context managers, exception handling statements, etc.\n",
    "\n",
    "The modified pattern would have to add positive lookbehind tokens in the regular expression, and accept whitespace characters of 2 or 4. The parser would also have to ensure the number of spaces in the indentation is consistent throughout the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
